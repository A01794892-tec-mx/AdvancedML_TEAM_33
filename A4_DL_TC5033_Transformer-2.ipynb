{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "#### Activity 4: Implementing a Translator\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "## TEAM 33\n",
    "| ID          | Name                          |\n",
    "|-------------|-------------------------------|\n",
    "| A01794956   | Luis Oporto                   |\n",
    "| A00915255   | Ernesto Zapata                |\n",
    "| A01065270   | José Manuel Romo Peredo       |\n",
    "| A01795483   | Nicolás Araque                |\n",
    "| A01794892   | Eduardo Rodriguez Ramirez     |\n",
    "\n",
    "- Objective\n",
    "\n",
    "To understand the Transformer Architecture by Implementing a translator.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n",
    "\n",
    "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
    "  \n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Traning a translator\n",
    "    - Translating at least 10 sentences.\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f54c65",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Script to convert csv to text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f02c0c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#This script requires to convert the TSV file to CSV\n",
    "# easiest way is to open it in Calc or excel and save as csv\n",
    "PATH = './data/input_spa_eng.csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c737d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2481</th>\n",
       "      <th>¡Intentemos algo!</th>\n",
       "      <th>1276</th>\n",
       "      <th>Let's try something.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2482</td>\n",
       "      <td>Tengo que irme a dormir.</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2483</td>\n",
       "      <td>¿Qué estás haciendo?</td>\n",
       "      <td>16492.0</td>\n",
       "      <td>What are you doing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2483</td>\n",
       "      <td>¿Qué estás haciendo?</td>\n",
       "      <td>241947.0</td>\n",
       "      <td>What are you doing now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2483</td>\n",
       "      <td>¿Qué estás haciendo?</td>\n",
       "      <td>516639.0</td>\n",
       "      <td>What are you up to?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2484</td>\n",
       "      <td>¿Qué es eso?</td>\n",
       "      <td>42849.0</td>\n",
       "      <td>What is it?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2481         ¡Intentemos algo!      1276     Let's try something.\n",
       "0  2482  Tengo que irme a dormir.    1277.0   I have to go to sleep.\n",
       "1  2483      ¿Qué estás haciendo?   16492.0      What are you doing?\n",
       "2  2483      ¿Qué estás haciendo?  241947.0  What are you doing now?\n",
       "3  2483      ¿Qué estás haciendo?  516639.0      What are you up to?\n",
       "4  2484              ¿Qué es eso?   42849.0              What is it?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787d9408",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMD RYZEN 7\\AppData\\Local\\Temp\\ipykernel_22364\\583859432.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n"
     ]
    }
   ],
   "source": [
    "eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()  \n",
    "eng_spa_cols = eng_spa_cols.sort_values(by='length')  \n",
    "eng_spa_cols = eng_spa_cols.drop(columns=['length'])  \n",
    "\n",
    "output_file_path = './data/eng-spa4.txt'\n",
    "eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {},
   "source": [
    "### Transformer - Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94241fb1",
   "metadata": {},
   "source": [
    "This portion of code contains the key components of our Transformer model for translation:\n",
    "\n",
    "- **PositionalEmbedding:** class creates positional encodings using sine and cosine functions, which are added to the input to include positional information.\n",
    "\n",
    "- **MultiHeadAttention:** class computes attention by transforming inputs into queries, keys, and values. It processes these through multiple heads, calculates attention scores, and combines the results.\n",
    "\n",
    "- **PositionFeedForward:** class is a simple two-layer network with a ReLU activation in between, transforming the input for further processing.\n",
    "\n",
    "- **EncoderSubLayer:** combines self-attention and a feedforward network, with residual connections, normalization, and dropout to refine the input representation.\n",
    "\n",
    "- **Encoder:** stacks multiple `EncoderSubLayer` instances to process the input sequence.\n",
    "\n",
    "- **DecoderSubLayer:** adds cross-attention to the encoder output alongside self-attention and a feedforward network.\n",
    "\n",
    "- **Decoder:** stacks these sub-layers to generate the final sequence, using the encoder's output as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5dcf681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1acd3b84c70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2cbd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c6623a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(self.pos_embed_matrix.shape)\n",
    "#         print(x.shape)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n",
    "        '''\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "    \n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a848f",
   "metadata": {},
   "source": [
    "Next portion of code defines the *Transformer Class* which implements the encoder, decoder, and supporting components defined on the last portion.\n",
    "\n",
    "It also defines the following methods:\n",
    "\n",
    "- **__init__** initializes the embeddings for input and target sequences, positional encodings, the encoder and decoder, and a linear layer for the final output.\n",
    "\n",
    "- **forward** processes the input and target sequences. It applies embedding and positional encoding, passes the source through the encoder, and processes the target using the decoder and encoder output. The final result is projected through the linear layer.\n",
    "\n",
    "- **mask** generates masks to handle padding and ensure valid sequence processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61070162",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Decoder embedding and postional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def mask(self, source, target):\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        size = target.size(1)\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask\n",
    "        return source_mask, target_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d40581d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len_source = 10\n",
    "seq_len_target = 10\n",
    "batch_size = 2\n",
    "input_vocab_size = 50\n",
    "target_vocab_size = 50\n",
    "\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc7cf689",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                  input_vocab_size, target_vocab_size, \n",
    "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4618560e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = model(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab0bc69d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouput.shape torch.Size([2, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
    "print(f'ouput.shape {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2910",
   "metadata": {},
   "source": [
    "### Translator Eng-Spa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd86a6",
   "metadata": {},
   "source": [
    "This section defines a set of functions we will use to processes text data and prepare it for use during training.\n",
    "\n",
    "- **preprocess_sentence:** normalizes a sentence by removing accents, converting to lowercase, stripping extra spaces, and adding start and end markers.\n",
    "\n",
    "- **build_vocab:** creates mappings between words and indices. It counts word occurrences and assigns indices, reserving specific indices for padding and unknown words.\n",
    "\n",
    "- **collate_fn:** prepares batches by truncating sequences to a maximum length, padding them to uniform size, and returning the batched data.\n",
    "\n",
    "It also defines the **EngSpaDataset** class which manages pairs of input and output sentences. It provides the dataset size and converts sentences into sequences of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "869a7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/eng-spa4.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0af1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c930226f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['No.', 'No.'],\n",
       " ['Sé.', 'I know.'],\n",
       " ['¿Y?', 'So?'],\n",
       " ['No.', 'Nay.'],\n",
       " ['Ya.', 'Come now.'],\n",
       " ['Ay.', 'Alas.'],\n",
       " ['¿Y?', 'And?'],\n",
       " ['Sí.', 'Yes.'],\n",
       " ['No.', 'Nope.'],\n",
       " ['Sí.', 'Yep.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095f4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [pair[1] for pair in eng_spa_pairs]\n",
    "spa_sentences = [pair[0] for pair in eng_spa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9e1c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No.', 'I know.', 'So?', 'Nay.', 'Come now.', 'Alas.', 'And?', 'Yes.', 'Nope.', 'Yep.']\n",
      "['No.', 'Sé.', '¿Y?', 'No.', 'Ya.', 'Ay.', '¿Y?', 'Sí.', 'No.', 'Sí.']\n"
     ]
    }
   ],
   "source": [
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d11478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478f673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '¿Hola @ cómo estás? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ac79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hola @ cómo estás? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "print(s1)\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9fc9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7a3b18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> no <eos>',\n",
       " '<sos> se <eos>',\n",
       " '<sos> y <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> ya <eos>',\n",
       " '<sos> ay <eos>',\n",
       " '<sos> y <eos>',\n",
       " '<sos> si <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> si <eos>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97931cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    word_count = Counter(words)\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fa8738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "spa_vocab_size = len(spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79d6b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27648 46929\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e564017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        self.eng_sentences = eng_sentences\n",
    "        self.spa_sentences = spa_sentences\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.spa_word2idx = spa_word2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.eng_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "        # return tokens idxs\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "        \n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b579577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
    "    return eng_batch, spa_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fef973",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8abf4",
   "metadata": {},
   "source": [
    "> During training we faced an 'Unkownw' CUDA error so we implemented save-reload functions along with checkpoint logic to resume trainning in case of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c392e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load checkpoint functions\n",
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path=\"checkpoint.pth\"):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Checkpoint loaded, resuming from epoch {start_epoch}\")\n",
    "    return start_epoch\n",
    "\n",
    "def save_model(model, path=\"model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(\"Final model saved.\")\n",
    "\n",
    "def load_model(model, path=\"model.pth\"):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(\"Model loaded for evaluation.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5d60eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimiser, epochs, checkpoint_path=\"checkpoint.pth\"):\n",
    "    model.train()\n",
    "    start_epoch = 0  # Default start epoch\n",
    "\n",
    "    # Check if there is an existing checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch = load_checkpoint(model, optimiser, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_loss = 0 \n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "            \n",
    "            # Decoder preprocessing\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Run model with source and target inputs\n",
    "            try:\n",
    "                output = model(eng_batch, target_input)\n",
    "            except TypeError as e:\n",
    "                print(f\"Error during model forward pass: {e}\")\n",
    "                return  # Stop training if there's an error\n",
    "\n",
    "            output = output.view(-1, output.size(-1))\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(output, target_output)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Average loss for this epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint at the end of each epoch\n",
    "        save_checkpoint(model, optimiser, epoch, checkpoint_path)\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2379ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e08eef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1181a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14e265e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 3.6003\n",
      "Checkpoint saved at epoch 0\n",
      "Epoch: 1/10, Loss: 2.2022\n",
      "Checkpoint saved at epoch 1\n",
      "Epoch: 2/10, Loss: 1.7029\n",
      "Checkpoint saved at epoch 2\n",
      "Epoch: 3/10, Loss: 1.3767\n",
      "Checkpoint saved at epoch 3\n",
      "Epoch: 4/10, Loss: 1.1283\n",
      "Checkpoint saved at epoch 4\n",
      "Epoch: 5/10, Loss: 0.9259\n",
      "Checkpoint saved at epoch 5\n",
      "Epoch: 6/10, Loss: 0.7613\n",
      "Checkpoint saved at epoch 6\n",
      "Epoch: 7/10, Loss: 0.6327\n",
      "Checkpoint saved at epoch 7\n",
      "Epoch: 8/10, Loss: 0.5381\n",
      "Checkpoint saved at epoch 8\n",
      "Epoch: 9/10, Loss: 0.4694\n",
      "Checkpoint saved at epoch 9\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader, loss_function, optimiser, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25975979",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb923a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMD RYZEN 7\\AppData\\Local\\Temp\\ipykernel_22364\\1160330894.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "#Reloading Model from file\n",
    "\n",
    "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1).to(device)\n",
    "\n",
    "model = load_model(model, path=\"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4bf61",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e376cd",
   "metadata": {},
   "source": [
    "This section defines functions utilized for conversion of sentences to indices, translation of a sentence, and evaluation of multiple translations using the trained model.\n",
    "\n",
    "- **sentence_to_indices** converts a sentence into a list of indices based on a provided word-to-index mapping. Words not in the mapping are replaced with the index for `<unk>`.\n",
    "\n",
    "- **indices_to_sentence** converts a list of indices back into a sentence using an index-to-word mapping. Padding tokens (`<pad>`) are excluded.\n",
    "\n",
    "- **translate_sentence** generates a translation for an input sentence using a trained model. It preprocesses the input, converts it to indices, and iteratively predicts the next token until the end-of-sequence (`<eos>`) token is reached or the maximum sequence length is exceeded.\n",
    "\n",
    "- **evaluate_translations** loops through a list of input sentences, translates each one, and prints both the input and the translated output.\n",
    "\n",
    "It also includes an example of testing translations with predefined sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50740746",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
    "\n",
    "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    model.eval()\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize the target tensor with <sos> token\n",
    "    tgt_indices = [spa_word2idx['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "            output = output.squeeze(0)\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "            if next_token == spa_word2idx['<eos>']:\n",
    "                break\n",
    "\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2c0db72",
   "metadata": {
    "code_folding": [
     15
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hello, how are you?\n",
      "Traducción: <sos> hola que tal <eos>\n",
      "\n",
      "Input sentence: I am learning artificial intelligence.\n",
      "Traducción: <sos> estoy aprendiendo la inteligencia artificial <eos>\n",
      "\n",
      "Input sentence: Artificial intelligence is great.\n",
      "Traducción: <sos> la inteligencia artificial es una gran critica <eos>\n",
      "\n",
      "Input sentence: Good night!\n",
      "Traducción: <sos> buenas noches <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    for sentence in sentences:\n",
    "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
    "        print(f'Input sentence: {sentence}')\n",
    "        print(f'Traducción: {translation}')\n",
    "        print()\n",
    "\n",
    "# Example sentences to test the translator\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning artificial intelligence.\",\n",
    "    \"Artificial intelligence is great.\",\n",
    "    \"Good night!\"\n",
    "]\n",
    "\n",
    "# Assuming the model is trained and loaded\n",
    "# Set the device to 'cpu' or 'cuda' as needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate translations\n",
    "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9873dfb",
   "metadata": {},
   "source": [
    "In this section we present 3 different test to asess translation quality of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16516",
   "metadata": {},
   "source": [
    "1. Poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ceefe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "poem=[\n",
    "    'I want you to know',\n",
    "    'one thing.',\n",
    "    'You know how this is:',\n",
    "    'if I look',\n",
    "    'at the crystal moon, at the red branch',\n",
    "    'of the slow autumn at my window,',\n",
    "    'if I touch',\n",
    "    'near the fire',\n",
    "    'the impalpable ash',\n",
    "    'or the wrinkled body of the log,',\n",
    "    'everything carries me to you,',\n",
    "    'as if everything that exists,',\n",
    "    'aromas, light, metals,',\n",
    "    'were little boats',\n",
    "    'that sail',\n",
    "    'toward those isles of yours that wait for me.'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a3a8af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: I want you to know\n",
      "Traducción: <sos> quiero conocerte <eos>\n",
      "\n",
      "Input sentence: one thing.\n",
      "Traducción: <sos> una cosa <eos>\n",
      "\n",
      "Input sentence: You know how this is:\n",
      "Traducción: <sos> tu sabes como es esto <eos>\n",
      "\n",
      "Input sentence: if I look\n",
      "Traducción: <sos> si estoy mira <eos>\n",
      "\n",
      "Input sentence: at the crystal moon, at the red branch\n",
      "Traducción: <sos> la luna roja se encuentra en la luna rojo <eos>\n",
      "\n",
      "Input sentence: of the slow autumn at my window,\n",
      "Traducción: <sos> mi ventana lenta a la ventana del oto o <eos>\n",
      "\n",
      "Input sentence: if I touch\n",
      "Traducción: <sos> si yo toco <eos>\n",
      "\n",
      "Input sentence: near the fire\n",
      "Traducción: <sos> el fuego se cerca <eos>\n",
      "\n",
      "Input sentence: the impalpable ash\n",
      "Traducción: <sos> el tanque pierde agua <eos>\n",
      "\n",
      "Input sentence: or the wrinkled body of the log,\n",
      "Traducción: <sos> el cuerpo o el tronco del cuerpo <eos>\n",
      "\n",
      "Input sentence: everything carries me to you,\n",
      "Traducción: <sos> me lleva todo a mi <eos>\n",
      "\n",
      "Input sentence: as if everything that exists,\n",
      "Traducción: <sos> eso existe como si todo existe <eos>\n",
      "\n",
      "Input sentence: aromas, light, metals,\n",
      "Traducción: <sos> la luz chirrio da luz <eos>\n",
      "\n",
      "Input sentence: were little boats\n",
      "Traducción: <sos> eran pocos botes <eos>\n",
      "\n",
      "Input sentence: that sail\n",
      "Traducción: <sos> navegar <eos>\n",
      "\n",
      "Input sentence: toward those isles of yours that wait for me.\n",
      "Traducción: <sos> esa persona tiene muchas ganas de que me esperen hacia el tuyo <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_translations(model, poem, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8090b1",
   "metadata": {},
   "source": [
    "2. Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "877c05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "song=[\n",
    "    \"This is the place\",\n",
    "    \"Sit down, you're safe now\",\n",
    "    \"You've been stuck in a lift\",\n",
    "    \"We've been trying to reach you, Thom\",\n",
    "    \"This is the place\",\n",
    "    \"It won't hurt ever again\",\n",
    "    \"The smell of air conditioning\",\n",
    "    \"The fish are belly-up\",\n",
    "    \"Empty all your pockets\",\n",
    "    \"'Cause it's time to come home\",\n",
    "    \"This is the place\",\n",
    "    \"Remember me\",\n",
    "    \"I'm the face you always see\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b294e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: This is the place\n",
      "Traducción: <sos> este es el lugar <eos>\n",
      "\n",
      "Input sentence: Sit down, you're safe now\n",
      "Traducción: <sos> ahora estas a salvo <eos>\n",
      "\n",
      "Input sentence: You've been stuck in a lift\n",
      "Traducción: <sos> has estado atascado en un lio <eos>\n",
      "\n",
      "Input sentence: We've been trying to reach you, Thom\n",
      "Traducción: <sos> hemos estado intentando llegar a la persona <eos>\n",
      "\n",
      "Input sentence: This is the place\n",
      "Traducción: <sos> este es el lugar <eos>\n",
      "\n",
      "Input sentence: It won't hurt ever again\n",
      "Traducción: <sos> no volvera nunca <eos>\n",
      "\n",
      "Input sentence: The smell of air conditioning\n",
      "Traducción: <sos> el aire acondicionado de la carretera <eos>\n",
      "\n",
      "Input sentence: The fish are belly-up\n",
      "Traducción: <sos> los peces estan subiendo <eos>\n",
      "\n",
      "Input sentence: Empty all your pockets\n",
      "Traducción: <sos> vacia todas tus bolsillos <eos>\n",
      "\n",
      "Input sentence: 'Cause it's time to come home\n",
      "Traducción: <sos> es hora de venir a casa <eos>\n",
      "\n",
      "Input sentence: This is the place\n",
      "Traducción: <sos> este es el lugar <eos>\n",
      "\n",
      "Input sentence: Remember me\n",
      "Traducción: <sos> acuerdate de mi <eos>\n",
      "\n",
      "Input sentence: I'm the face you always see\n",
      "Traducción: <sos> siempre ves la cara que estoy viendo <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_translations(model, song, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685dd799",
   "metadata": {},
   "source": [
    "3. Scienttific text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07d9f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=[\n",
    "    \"Real-time AI on edge devices refers to the deployment of AI models on local hardware.\",\n",
    "    \"These devices can include IoT devices, smartphones, and other hardware.\",\n",
    "    \"One goal of deploying AI on edge devices is to reduce latency.\",\n",
    "    \"Another goal is to reduce dependency on cloud computing.\",\n",
    "    \"Techniques used for edge deployment include model compression.\",\n",
    "    \"Quantization is another technique employed for this purpose.\",\n",
    "    \"Pruning is also used to make models more suitable for edge deployment.\",\n",
    "    \"These techniques make the models lightweight and efficient.\",\n",
    "    \"Edge-deployed models can operate with lower power consumption.\",\n",
    "    \"This approach enables AI to function effectively on limited hardware.\",\n",
    "    \"Real-time AI on edge is applied in various smart devices.\",\n",
    "    \"It is also used in autonomous vehicles.\",\n",
    "    \"Industrial IoT uses edge AI for real-time processing.\",\n",
    "    \"One application is object detection in surveillance systems.\",\n",
    "    \"Predictive maintenance in manufacturing also benefits from edge AI.\",\n",
    "    \"Another application is natural language processing on mobile devices.\",\n",
    "    \"Edge AI allows for quicker responses in real-time applications.\",\n",
    "    \"It also enhances privacy for users.\",\n",
    "    \"Data does not need to be sent to the cloud in this setup.\",\n",
    "    \"This approach keeps processing closer to the data source.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c84a1d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Real-time AI on edge devices refers to the deployment of AI models on local hardware.\n",
      "Traducción: <sos> la ia laboral de la ia permite matrimonios en los derechos humanos en el limite de la ciudad <eos>\n",
      "\n",
      "Input sentence: These devices can include IoT devices, smartphones, and other hardware.\n",
      "Traducción: <sos> estas dispositivos se pueden cambiar de materia y dispositivos <eos>\n",
      "\n",
      "Input sentence: One goal of deploying AI on edge devices is to reduce latency.\n",
      "Traducción: <sos> reducir es uno de los tipos de sueldo para reducir un limite sin embargo <eos>\n",
      "\n",
      "Input sentence: Another goal is to reduce dependency on cloud computing.\n",
      "Traducción: <sos> el objetivo de reducir la sangre es reducir la revision <eos>\n",
      "\n",
      "Input sentence: Techniques used for edge deployment include model compression.\n",
      "Traducción: <sos> modelo solia comprar tropas para la taquilla <eos>\n",
      "\n",
      "Input sentence: Quantization is another technique employed for this purpose.\n",
      "Traducción: <sos> esta tecnica es muy controvertida para otra tecnica <eos>\n",
      "\n",
      "Input sentence: Pruning is also used to make models more suitable for edge deployment.\n",
      "Traducción: <sos> el limite usado es usado para mas modelos que tambien interpretaba para hacer modelos <eos>\n",
      "\n",
      "Input sentence: These techniques make the models lightweight and efficient.\n",
      "Traducción: <sos> estos modelos basicos y hacen falta el tono <eos>\n",
      "\n",
      "Input sentence: Edge-deployed models can operate with lower power consumption.\n",
      "Traducción: <sos> las modelos de poder pueden provocar modelos con fuerza de poder mas baja <eos>\n",
      "\n",
      "Input sentence: This approach enables AI to function effectively on limited hardware.\n",
      "Traducción: <sos> esta funcion permite evitar medicamentos infantil en la funcion infantil <eos>\n",
      "\n",
      "Input sentence: Real-time AI on edge is applied in various smart devices.\n",
      "Traducción: <sos> la ia esta en distintos tiempo del real madrid el de un antiguo aumento antiguo <eos>\n",
      "\n",
      "Input sentence: It is also used in autonomous vehicles.\n",
      "Traducción: <sos> tambien se usa en total <eos>\n",
      "\n",
      "Input sentence: Industrial IoT uses edge AI for real-time processing.\n",
      "Traducción: <sos> la ia usa el tiempo real para la ira bien <eos>\n",
      "\n",
      "Input sentence: One application is object detection in surveillance systems.\n",
      "Traducción: <sos> una aplicacion de sistemas de aviones esta en sistemas <eos>\n",
      "\n",
      "Input sentence: Predictive maintenance in manufacturing also benefits from edge AI.\n",
      "Traducción: <sos> la ia tambien se vende beneficios de los beneficios de las ganancias en mantenimiento <eos>\n",
      "\n",
      "Input sentence: Another application is natural language processing on mobile devices.\n",
      "Traducción: <sos> el pastel del idioma humano es otro planeta natural <eos>\n",
      "\n",
      "Input sentence: Edge AI allows for quicker responses in real-time applications.\n",
      "Traducción: <sos> las aplicaciones informaticas permite la ia de manera real en durante el tiempo real <eos>\n",
      "\n",
      "Input sentence: It also enhances privacy for users.\n",
      "Traducción: <sos> los usuarios tambien lo hacen por la intimidad <eos>\n",
      "\n",
      "Input sentence: Data does not need to be sent to the cloud in this setup.\n",
      "Traducción: <sos> los datos no necesitan ser enviados a esta nube <eos>\n",
      "\n",
      "Input sentence: This approach keeps processing closer to the data source.\n",
      "Traducción: <sos> esta fuente sigue mas cerca de la ciencia datos de los datos <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_translations(model, txt, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2d58a",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4360fc2",
   "metadata": {},
   "source": [
    "\n",
    "The model demonstrates some basic structure recognition in translations but lacks consistency, especially with complex language across different types of text.\n",
    "\n",
    "In the poem translation, the model captures a few simple phrases but generally struggles with the nuances and subtleties. The translations are often inaccurate.\n",
    "\n",
    "As for the song, the model performs slightly better. It translates straightforward phrases correctly, handling basic sentence structures with relative success. However, it falters with idiomatic expressions as 'belly-up', resulting in translations that miss the intended tone or metaphorical meaning.\n",
    "\n",
    "In the scientific text translation, the model shows an attempt to handle technical terms, occasionally producing accurate vocabulary. Despite this, it frequently misinterprets specialized language, leading to confusing or irrelevant translations. This indicates that the model could benefit from more training on technical and domain-specific vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
